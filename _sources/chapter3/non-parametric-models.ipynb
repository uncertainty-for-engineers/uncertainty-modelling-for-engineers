{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric models\n",
    "\n",
    "Bayesian non-parametric models are models for regression which learn a\n",
    "prior distribution over functions at training time. This is achieved by\n",
    "learning so-called 'Kernel hyper-parameters' from the training data set,\n",
    "which specify a Gaussian Process Prior. Inference is performed at test\n",
    "time by applying Bayes' law to this prior with the available data\n",
    "{cite}`diaz2011gaussian`. The process of performing computation inference at\n",
    "test time is referred to as lazy learning (as opposed to eager learning\n",
    "presented in the previous section, where the inference happens during\n",
    "training, and only a single pass through the network is required to make\n",
    "predictions). Gaussian Process models can be tuned to have desirable\n",
    "properties for many applications, for example one can assume that the\n",
    "training data is noise-free and hence the relevant function can be\n",
    "learned from very few samples. Gaussian Processes are particularly\n",
    "useful for global optimisation of non-linear functions, because the\n",
    "predictive uncertainty can be used to decide where the next sample\n",
    "should be chosen {cite}`frazier2018tutorial`. Gaussian processes are not used\n",
    "in this thesis (except for as a comparison in some of the numerical\n",
    "examples), but we will briefly describe how they relate to the Bayesian\n",
    "parametric models discussed in this Chapter.\n",
    "\n",
    "It can be shown that a single layer Bayesian neural network, with an\n",
    "infinite number of neurons in the layer, is equivalent to a Gaussian\n",
    "Process, and can therefore be used as a more convenient alternative,\n",
    "since the inference is performed at training time {cite}`neal2012bayesian`.\n",
    "Variational approximations can be made for Gaussian Processes to make\n",
    "the inference computationally tractable for large data sets and deep\n",
    "architectures {cite}`hensman2013gaussian`. Neural Processes learn a\n",
    "distribution over functions in a similar way to Gaussian processes, but\n",
    "with significantly reduced computational expense since only a forward\n",
    "pass through the neural network is required at test time\n",
    "{cite}`garnelo2018neural`. Neural Processes are useful for meta-learning\n",
    "(learning to learn). For example, they were used to learn how to predict\n",
    "2D views of 3D spaces, given limited training data {cite}`eslami2018neural`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}